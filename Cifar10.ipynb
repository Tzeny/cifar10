{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Tzeny/cifar10/blob/master/Cifar10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "abWGmIbfhggR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "26fb57c8-dbf7-45f4-8464-ba168a2867bd"
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/45/99/837428d26b47ebd6b66d6e1b180e98ec4a557767a93a81a02ea9d6242611/GPUtil-1.3.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.5)\n",
            "Building wheels for collected packages: gputil\n",
            "  Running setup.py bdist_wheel for gputil ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/17/0f/04/b79c006972335e35472c0b835ed52bfc0815258d409f560108\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.3.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.6)\n",
            "Collecting humanize\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/e0/e512e4ac6d091fc990bbe13f9e0378f34cf6eecd1c6c268c9e598dcf5bb9/humanize-0.5.1.tar.gz\n",
            "Building wheels for collected packages: humanize\n",
            "  Running setup.py bdist_wheel for humanize ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/69/86/6c/f8b8593bc273ec4b0c653d3827f7482bb2001a2781a73b7f44\n",
            "Successfully built humanize\n",
            "Installing collected packages: humanize\n",
            "Successfully installed humanize-0.5.1\n",
            "Gen RAM Free: 9.5 GB  | Proc size: 3.7 GB\n",
            "GPU RAM Free: 503MB | Used: 10936MB | Util  96% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6U2APHXm65Q4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3yjSJHMh-RR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bbed0632-b4a9-457f-fecb-2edd1f799527"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hGW67WZwiBO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d997d2df-d0ec-48d7-edb8-c3eed28958bd"
      },
      "cell_type": "code",
      "source": [
        "#gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "x_train_gray = np.dot(x_train[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "x_test_gray = np.dot(x_test[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "plt.imshow(x_train_gray[1], cmap='gray')\n",
        "plt.imshow(x_train[1])\n",
        "\n",
        "x_train_gray = x_train_gray.reshape(-1,32,32,1)\n",
        "x_test_gray = x_test_gray.reshape(-1,32,32,1)\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "i=0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ffa76f121d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KqmcChpEiFJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from keras.layers import Flatten, Activation, Conv2D, MaxPool2D, AvgPool2D, Dense, Dropout, BatchNormalization, Input, MaxPooling2D, Flatten, Activation, Conv2D, AvgPool2D, Dense, Dropout, concatenate\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.models import model_from_json, Model\n",
        "\n",
        "def build_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                dropout=False, normalization=False, regularization=\"l2\", dropout_ratio=0.25):\n",
        "    #3x3 kernel tower\n",
        "    tower = Conv2D(features_nr, (1,1), padding='same', activation='relu', \n",
        "                     kernel_regularizer=regularization, name='tower_%d_%dx%da'%(tower_nr, shape[0], shape[1]))(input_layer)\n",
        "    tower = Conv2D(features_nr*2, shape, padding='same', activation='relu',\n",
        "                     kernel_regularizer=regularization, name='tower_%d_%dx%db'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    #condidional dropout/normalization\n",
        "    if dropout:\n",
        "        tower = Dropout(dropout_ratio, name='tower_%d_%dx%ddrop'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    if normalization:\n",
        "        tower = BatchNormalization(name='tower_%d_%dx%dnorm'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "        \n",
        "    return tower\n",
        "\n",
        "def build_simple_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                dropout=False, normalization=False, regularization=\"l2\", dropout_ratio=0.25):\n",
        "    #3x3 kernel tower\n",
        "    tower = Conv2D(features_nr, shape, padding='same', activation='relu',\n",
        "                     kernel_regularizer=regularization, \n",
        "                   name='tower_simple_%d_%dx%db'%(tower_nr, shape[0], shape[1]))(input_layer)\n",
        "    #condidional dropout/normalization\n",
        "    if dropout:\n",
        "        tower = Dropout(dropout_ratio, name='tower_%d_%dx%ddrop'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    if normalization:\n",
        "        tower = BatchNormalization(name='tower_%d_%dx%dnorm'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "        \n",
        "    return tower\n",
        "\n",
        "def build_tower_subsample(input_layer, features_nr, shape, tower_nr, \n",
        "                          dropout=False, normalization=False, regularization='l2', dropout_ratio=0.25):\n",
        "    tower = build_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                        dropout, normalization, regularization, dropout_ratio)\n",
        "    pool = MaxPooling2D((2,2), padding='same', name='tower_%d_2x2subsample'%(tower_nr))(tower)\n",
        "\n",
        "    return pool\n",
        "\n",
        "def build_simple_tower_subsample(input_layer, features_nr, shape, tower_nr, \n",
        "                          dropout=False, normalization=False, regularization='l2', dropout_ratio=0.25):\n",
        "    tower = build_simple_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                        dropout, normalization, regularization, dropout_ratio)\n",
        "    pool = MaxPooling2D((2,2), padding='same', name='tower_%d_2x2subsample'%(tower_nr))(tower)\n",
        "\n",
        "    return pool\n",
        "\n",
        "def build_dense(input_layer, neurons_nr, dense_nr, \n",
        "                dropout=False, normalization=False, regularization='l2', dropout_ratio=0.5):\n",
        "    dense = Dense(neurons_nr, kernel_regularizer=regularization, \n",
        "                  name='dense_%d_%d'%(dense_nr, neurons_nr))(input_layer)\n",
        "    \n",
        "    if dropout:\n",
        "        dense = Dropout(dropout_ratio, name='dense_%d_%ddrop'%(dense_nr, neurons_nr))(dense)\n",
        "    if normalization:\n",
        "        dense = BatchNormalization(name='dense_%d_%dnorm'%(dense_nr, neurons_nr))(dense)\n",
        "    \n",
        "    return dense\n",
        "\n",
        "def build_inception_module(input_layer, features_nr, module_nr, \n",
        "                           dropout=False, normalization=False, regularization='l2', dropout_ratio=0.2):  \n",
        "    tower_1 = build_tower(input_layer, features_nr, (3,3), module_nr, \n",
        "                          dropout, normalization, regularization, dropout_ratio)\n",
        "\n",
        "    tower_2 = build_tower(input_layer, features_nr, (5,5), module_nr, \n",
        "                          dropout, normalization, regularization, dropout_ratio)\n",
        "\n",
        "    #max pooling tower\n",
        "    tower_3 = MaxPooling2D((3,3), strides=(1,1), padding='same', name='inception_%d_pool3x3a'%(module_nr))(input_layer)\n",
        "    tower_3 = Conv2D(features_nr*2, (1,1), padding='same', activation='relu',\n",
        "                    kernel_regularizer=regularization, name='inception_%d_pool3x3b'%(module_nr))(tower_3)\n",
        "    if dropout:\n",
        "        tower_3 = Dropout(dropout_ratio, name='inception_%d_pool3x3drop'%(module_nr))(tower_3)\n",
        "    if normalization:\n",
        "        tower_3 = BatchNormalization(name='inception_%d_pool3x3norm'%(module_nr))(tower_3)\n",
        "\n",
        "    #concatenate and subsample towers\n",
        "    output = concatenate([tower_1, tower_2, tower_3], axis = 3, name='inception_%d_concat'%(module_nr))\n",
        "\n",
        "    pooled = MaxPooling2D((2,2), padding='same', name='inception_%d_2x2subsample'%(module_nr))(output)\n",
        "    \n",
        "    return pooled\n",
        "\n",
        "i=datetime.datetime.now().strftime(\"%I:%M%p_%B-%d-%Y\")\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "!mkdir -p models\n",
        "!mkdir -p cifar10-logs\n",
        "\n",
        "a = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')#will stop the model if val_loss does not improve for 2 consecutive epochs\n",
        "b = ModelCheckpoint(monitor='val_loss', filepath='./models/cifar10-'+str(i)+'.hdf5', verbose=1, save_best_only=True)#save model weights after each epoch if val_loss improves\n",
        "c = TensorBoard(log_dir='./cifar10-logs/'+str(i),\n",
        "                write_grads=True,\n",
        "                write_graph=True,\n",
        "                write_images=True,\n",
        "                batch_size=128)#saves a log file for tensorboard; remember to save different runs to different subdirectories\n",
        "\n",
        "#we'll use this instead of decay\n",
        "d = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "\n",
        "callbacks=[a,b,c,d]\n",
        "\n",
        "#------------model definition-------------------\n",
        "\n",
        "use_norm = True\n",
        "lrate = 0.001\n",
        "\n",
        "input_img = Input(shape = (32, 32, 3), name='input')\n",
        "\n",
        "#conv_1 = Conv2D(1, (1,1), padding='same', activation='relu', \n",
        "               # kernel_regularizer = regularization, name='conv_64x64x1_inception_in')(input_img)\n",
        "\n",
        "#hopefully this will learn a good internal representation of the image channels\n",
        "#conv_1 = Conv2D(1, (1,1), padding='same', activation='relu', \n",
        "                #kernel_regularizer = regularization, name='conv_64x64x1_inception_in')(input_img)\n",
        "\n",
        "inception_1 = build_inception_module(input_img, 3, 1, True, use_norm)\n",
        "\n",
        "inception_2 = build_inception_module(inception_1, 36, 2, True, use_norm)\n",
        "\n",
        "inception_3 = build_inception_module(inception_2, 144, 3, True, use_norm)\n",
        "\n",
        "#tower_3 = build_simple_tower(inception_2, 144, (3,3),  3, False, use_norm)\n",
        "#tower_4 = build_simple_tower_subsample(tower_3, 144, (3,3), 4, False, use_norm)\n",
        "\n",
        "tower_5 = build_simple_tower(inception_3, 288, (3,3),  5, False, use_norm)\n",
        "tower_6 = build_simple_tower_subsample(tower_5, 288, (3,3), 6, False, use_norm)\n",
        "\n",
        "#model top\n",
        "\n",
        "flat = Flatten()(tower_6)\n",
        "\n",
        "dense_5 = build_dense(flat, 128, 1, True, use_norm)\n",
        "\n",
        "dense_6 = build_dense(dense_5, 64, 2, True, use_norm)\n",
        "\n",
        "out = Dense(10, activation='softmax')(dense_6)\n",
        "\n",
        "model = Model(inputs = input_img, outputs = out)\n",
        "\n",
        "#-----------------------------------------------\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lrate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./models/cifar10-\"+str(i)+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"Saved model to\" + \"../models/cifar10-\"+str(i)+\".json\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oZQ5GrZVA1bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8fe518f-b3fe-40b9-ee5c-83bb54a491f5"
      },
      "cell_type": "code",
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    import numpy as np\n",
        "    from keras import backend as K\n",
        "\n",
        "    shapes_mem_count = 0\n",
        "    for l in model.layers:\n",
        "        single_layer_mem = 1\n",
        "        for s in l.output_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        shapes_mem_count += single_layer_mem\n",
        "\n",
        "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
        "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
        "\n",
        "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
        "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
        "    return gbytes\n",
        "  \n",
        "print(\"Memory usage (GB):\", get_model_memory_usage(128,model))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage (GB): 1.073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JRZf1eVHiKpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2101
        },
        "outputId": "df90bccd-6182-4a9a-c0ae-ec124270ff14"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  model.fit(x_train, y_train_cat, batch_size=128, epochs=100, validation_split=0.2,verbose=1,callbacks=callbacks)  # starts training\n",
        "\n",
        "result = model.evaluate(x_test, y_test_cat)\n",
        "\n",
        "print(\"Accuracy on test set: \",result[1]*100,\"%\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 41s 1ms/step - loss: 1.7731 - acc: 0.9018 - val_loss: 0.4834 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.48337, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 2/100\n",
            "17408/40000 [============>.................] - ETA: 19s - loss: 0.3830 - acc: 0.9085"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 911us/step - loss: 0.3594 - acc: 0.9100 - val_loss: 0.3691 - val_acc: 0.9029\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.48337 to 0.36906, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 3/100\n",
            "31872/40000 [======================>.......] - ETA: 6s - loss: 0.3176 - acc: 0.9141"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.3165 - acc: 0.9144 - val_loss: 0.3406 - val_acc: 0.9071\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.36906 to 0.34059, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 4/100\n",
            "35456/40000 [=========================>....] - ETA: 3s - loss: 0.3046 - acc: 0.9170"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 911us/step - loss: 0.3043 - acc: 0.9171 - val_loss: 0.3859 - val_acc: 0.8989\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.34059\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.2939 - acc: 0.9198 - val_loss: 0.3567 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.34059\n",
            "Epoch 6/100\n",
            " 2816/40000 [=>............................] - ETA: 31s - loss: 0.2917 - acc: 0.9219"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 914us/step - loss: 0.2874 - acc: 0.9218 - val_loss: 0.3253 - val_acc: 0.9153\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.34059 to 0.32533, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 7/100\n",
            "28288/40000 [====================>.........] - ETA: 9s - loss: 0.2843 - acc: 0.9224 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 914us/step - loss: 0.2839 - acc: 0.9223 - val_loss: 0.3854 - val_acc: 0.8937\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.32533\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 36s 911us/step - loss: 0.2788 - acc: 0.9237 - val_loss: 0.6566 - val_acc: 0.8351\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.32533\n",
            "Epoch 9/100\n",
            " 1536/40000 [>.............................] - ETA: 32s - loss: 0.2787 - acc: 0.9221"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 912us/step - loss: 0.2738 - acc: 0.9240 - val_loss: 0.2907 - val_acc: 0.9155\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.32533 to 0.29067, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 10/100\n",
            "27904/40000 [===================>..........] - ETA: 10s - loss: 0.2690 - acc: 0.9250"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 915us/step - loss: 0.2698 - acc: 0.9248 - val_loss: 0.3317 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.29067\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 36s 912us/step - loss: 0.2646 - acc: 0.9265 - val_loss: 0.2994 - val_acc: 0.9153\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.29067\n",
            "Epoch 12/100\n",
            " 1408/40000 [>.............................] - ETA: 32s - loss: 0.2609 - acc: 0.9271"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 912us/step - loss: 0.2656 - acc: 0.9261 - val_loss: 0.3661 - val_acc: 0.9026\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.29067\n",
            "Epoch 13/100\n",
            "34432/40000 [========================>.....] - ETA: 4s - loss: 0.2578 - acc: 0.9275"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.2581 - acc: 0.9273 - val_loss: 0.2919 - val_acc: 0.9191\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29067\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.2572 - acc: 0.9271 - val_loss: 0.2954 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29067\n",
            "Epoch 15/100\n",
            " 2432/40000 [>.............................] - ETA: 32s - loss: 0.2440 - acc: 0.9306"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 910us/step - loss: 0.2055 - acc: 0.9374 - val_loss: 0.2495 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.29067 to 0.24954, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 16/100\n",
            "28160/40000 [====================>.........] - ETA: 10s - loss: 0.1865 - acc: 0.9417"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1858 - acc: 0.9419 - val_loss: 0.2406 - val_acc: 0.9239\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.24954 to 0.24056, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 17/100\n",
            "34432/40000 [========================>.....] - ETA: 4s - loss: 0.1769 - acc: 0.9448"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1768 - acc: 0.9448 - val_loss: 0.2281 - val_acc: 0.9272\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.24056 to 0.22805, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 18/100\n",
            "35968/40000 [=========================>....] - ETA: 3s - loss: 0.1718 - acc: 0.9463"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 910us/step - loss: 0.1715 - acc: 0.9464 - val_loss: 0.2128 - val_acc: 0.9308\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.22805 to 0.21284, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 19/100\n",
            "36352/40000 [==========================>...] - ETA: 3s - loss: 0.1659 - acc: 0.9488"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 911us/step - loss: 0.1661 - acc: 0.9488 - val_loss: 0.2039 - val_acc: 0.9335\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.21284 to 0.20388, saving model to ./models/cifar10-06:43AM_July-07-2018.hdf5\n",
            "Epoch 20/100\n",
            "36480/40000 [==========================>...] - ETA: 3s - loss: 0.1623 - acc: 0.9504"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 912us/step - loss: 0.1623 - acc: 0.9504 - val_loss: 0.2263 - val_acc: 0.9257\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.20388\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 36s 910us/step - loss: 0.1588 - acc: 0.9521 - val_loss: 0.2170 - val_acc: 0.9292\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.20388\n",
            "Epoch 22/100\n",
            " 2944/40000 [=>............................] - ETA: 31s - loss: 0.1539 - acc: 0.9535"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 911us/step - loss: 0.1554 - acc: 0.9533 - val_loss: 0.2455 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.20388\n",
            "Epoch 23/100\n",
            "34944/40000 [=========================>....] - ETA: 4s - loss: 0.1527 - acc: 0.9546"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1527 - acc: 0.9544 - val_loss: 0.2301 - val_acc: 0.9268\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.20388\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 37s 914us/step - loss: 0.1500 - acc: 0.9559 - val_loss: 0.2649 - val_acc: 0.9157\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.20388\n",
            "Epoch 25/100\n",
            " 2688/40000 [=>............................] - ETA: 31s - loss: 0.1405 - acc: 0.9583"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1374 - acc: 0.9604 - val_loss: 0.2232 - val_acc: 0.9267\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.20388\n",
            "Epoch 26/100\n",
            "34816/40000 [=========================>....] - ETA: 4s - loss: 0.1322 - acc: 0.9622"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 36s 910us/step - loss: 0.1317 - acc: 0.9624 - val_loss: 0.2388 - val_acc: 0.9204\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.20388\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 36s 912us/step - loss: 0.1288 - acc: 0.9637 - val_loss: 0.2327 - val_acc: 0.9233\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.20388\n",
            "Epoch 28/100\n",
            " 2560/40000 [>.............................] - ETA: 31s - loss: 0.1294 - acc: 0.9636"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1271 - acc: 0.9637 - val_loss: 0.2342 - val_acc: 0.9223\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.20388\n",
            "Epoch 29/100\n",
            "34816/40000 [=========================>....] - ETA: 4s - loss: 0.1247 - acc: 0.9647"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 37s 913us/step - loss: 0.1250 - acc: 0.9647 - val_loss: 0.2379 - val_acc: 0.9208\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.20388\n",
            "Epoch 00029: early stopping\n",
            "10000/10000 [==============================] - 5s 498us/step\n",
            "Accuracy on test set:  92.13599914550781 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TqgxBsf7j3lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5d92ac3d-8515-4f20-c1f1-52ca07178897"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('./models/cifar10-06:43AM_July-07-2018.hdf5')\n",
        "\n",
        "result = model.evaluate(x_test, y_test_cat)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 578us/step\n",
            "[0.20430024523735046, 0.9331499870300293]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wziduBIfj9gW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}